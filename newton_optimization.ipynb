{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data_utils\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from torch.utils.data import DataLoader\n",
    "from model import WideModel\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/96rhddjx1b77w_4r1x86flk80000gn/T/ipykernel_62643/2578780564.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[\"Diagnosis\"] = y[\"Diagnosis\"].map({\"M\": 1, \"B\": 0})\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "# pip install ucimlrepo\n",
    "\n",
    "# fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = torch.tensor(breast_cancer_wisconsin_diagnostic.data.features.values, dtype=torch.float32)\n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets\n",
    "y[\"Diagnosis\"] = y[\"Diagnosis\"].map({\"M\": 1, \"B\": 0})\n",
    "y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "train = data_utils.TensorDataset(X[:455], y[:455])\n",
    "test = data_utils.TensorDataset(X[455:], y[455:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders. Batch size must be 1\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    }
   ],
   "source": [
    "# create device\n",
    "dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"using device\", dev)\n",
    "\n",
    "# create model\n",
    "model = WideModel(input_dim=30,hidden_dim_scale = 20, output_dim=1).to(dev)\n",
    "\n",
    "# create optimizer\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.SGD([p for p in model.parameters()], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3189eacb14164b1aae498139ac68b4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get linearized models:\n",
    "num_params = len(model.flatten_parameters())\n",
    "\n",
    "# we reduce f(x,w) to Aw+B, where there is a different A,B per x\n",
    "\n",
    "As = torch.empty((0,num_params))\n",
    "Bs = torch.empty((0,))\n",
    "ys = torch.empty((0,))\n",
    "\n",
    "for x,y in tqdm(train_dataloader):\n",
    "    x = x.to(dev)\n",
    "    \n",
    "    # A = gradient matrix of logits\n",
    "    A = model.flatten_gradient(x).unsqueeze(0)\n",
    "    # print(A.shape)\n",
    "    As = torch.concat((As, A), dim=0)\n",
    "    \n",
    "    # B = f(x,w) - A w\n",
    "    B = model.forward(x) - A @ model.flatten_parameters()\n",
    "    Bs = torch.concat((Bs, B[:,0]), dim=0)\n",
    "    \n",
    "    ys = torch.concat((ys, y[:,0]), dim=0)\n",
    "    \n",
    "    model.Adict[x] = A\n",
    "    model.Bdict[x] = B\n",
    "    \n",
    "model.update_stored_linear_tensors(As, Bs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.5066,  -0.6980,  -3.4752,  ...,  25.8849,  69.7248,   1.0000],\n",
       "        [ -0.4540,  -0.5858,  -2.8905,  ...,  21.2490,  55.1975,   1.0000],\n",
       "        [ -0.4959,  -0.6057,  -3.2478,  ...,  20.3505,  75.1512,   1.0000],\n",
       "        ...,\n",
       "        [ -0.4689,  -0.5620,  -2.9864,  ...,  21.1079,  61.0999,   1.0000],\n",
       "        [ -0.7197,  -0.8181,  -4.7032,  ...,  30.5737, 151.9301,   1.0000],\n",
       "        [ -0.4462,  -0.6774,  -2.9100,  ...,  21.4290,  55.0188,   1.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "As"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# change weights to be a normal scale\n",
    "\n",
    "f = model.batched_linearized_forward(model.w0)\n",
    "while f.abs().max() > 1:\n",
    "    print(\"updated\")\n",
    "    w = model.flatten_parameters()/2\n",
    "    model.update_parameters(w)\n",
    "    f = model.batched_linearized_forward(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d5959ca5014443b4078f7b4dcaf809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get linearized models:\n",
    "num_params = len(model.flatten_parameters())\n",
    "\n",
    "# we reduce f(x,w) to Aw+B, where there is a different A,B per x\n",
    "\n",
    "As = torch.empty((0,num_params))\n",
    "Bs = torch.empty((0,))\n",
    "ys = torch.empty((0,))\n",
    "\n",
    "for x,y in tqdm(train_dataloader):\n",
    "    x = x.to(dev)\n",
    "    \n",
    "    # A = gradient matrix of logits\n",
    "    A = model.flatten_gradient(x).unsqueeze(0)\n",
    "    # print(A.shape)\n",
    "    As = torch.concat((As, A), dim=0)\n",
    "    \n",
    "    # B = f(x,w) - A w\n",
    "    B = model.forward(x) - A @ model.flatten_parameters()\n",
    "    Bs = torch.concat((Bs, B[:,0]), dim=0)\n",
    "    \n",
    "    ys = torch.concat((ys, y[:,0]), dim=0)\n",
    "    \n",
    "    model.Adict[x] = A\n",
    "    model.Bdict[x] = B\n",
    "    \n",
    "model.update_stored_linear_tensors(As, Bs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0311, -0.0472, -0.1993,  ...,  1.4814,  4.3266,  1.0000],\n",
       "        [-0.0426, -0.0499, -0.2831,  ...,  1.7597,  9.0688,  1.0000],\n",
       "        [-0.0266, -0.0553, -0.1723,  ...,  1.2860,  3.2890,  1.0000],\n",
       "        ...,\n",
       "        [-0.0439, -0.0558, -0.2895,  ...,  1.9796,  8.7738,  1.0000],\n",
       "        [-0.0280, -0.0461, -0.1824,  ...,  1.3851,  3.3790,  1.0000],\n",
       "        [-0.0290, -0.0429, -0.1834,  ...,  1.3205,  3.7999,  1.0000]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0399, -0.0743,  0.0156,  ..., -0.0099,  0.0023, -0.0127],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Starting Newton step 0\n",
      "f is nan any: False\n",
      "dl[0], 15.01091480255127\n",
      "dl is nan any: False\n",
      "invddl[0], tensor([ 1.7200e-06,  7.0556e-06,  7.6109e-06,  ..., -3.0915e-05,\n",
      "         2.6660e-05, -2.7797e-06], grad_fn=<SelectBackward0>)\n",
      "invddl is nan any: False\n",
      "tensor([ 4.7132e+00,  7.5275e+01,  1.1787e+02,  ..., -6.4798e+03,\n",
      "         8.2259e+03,  3.7564e+03], grad_fn=<SubBackward0>)\n",
      "change in w is: 797832.0625\n"
     ]
    }
   ],
   "source": [
    "w = model.w0\n",
    "print(w)\n",
    "for step in range(1):\n",
    "    print(f\"Starting Newton step {step}\")\n",
    "    old_w = w\n",
    "    w = model.newton_update(w, ys)\n",
    "    print(w)\n",
    "    print(f\"change in w is: {torch.linalg.norm(w-old_w)}\")\n",
    "    # print(f\"Achieved loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = model.batched_linearized_forward(w)\n",
    "dl = (torch.exp(f)/(1+torch.exp(f)) - ys) @ model.Atensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.exp(f[3])/(1+torch.exp(f[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(784817.9375, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., nan, nan, nan, 0., 0., 0., nan, 0., 0., nan, 0., 0., nan, nan, nan, 0., 0., nan, 0., nan, 0.,\n",
       "        nan, 0., nan, 0., 0., 0., 0., 0., nan, nan, nan, 0., nan, nan, 0., 0., nan, nan, 0., nan, 0., nan, 0., nan,\n",
       "        0., 0., 0., nan, nan, 0., 0., nan, nan, nan, 0., 0., 0., nan, nan, 0., nan, nan, 0., 0., nan, 0., 0., nan,\n",
       "        0., nan, 0., nan, nan, 0., 0., 0., 0., 0., 0., nan, nan, 0., nan, nan, 0., nan, 0., nan, 0., nan, 0., 0.,\n",
       "        0., 0., 0., 0., nan, 0., 0., 0., 0., 0., nan, nan, nan, 0., nan, nan, 0., nan, nan, nan, 0., nan, nan, nan,\n",
       "        0., 0., nan, 0., nan, 0., nan, nan, nan, nan, nan, 0., nan, 0., nan, 0., 0., 0., 0., nan, nan, nan, nan, 0.,\n",
       "        0., 0., nan, 0., 0., nan, nan, nan, 0., nan, nan, 0., 0., nan, 0., 0., 0., nan, nan, 0., nan, nan, 0., 0.,\n",
       "        nan, nan, nan, nan, 0., 0., nan, 0., nan, 0., 0., nan, 0., nan, nan, 0., nan, nan, 0., 0., nan, nan, 0., nan,\n",
       "        nan, nan, nan, nan, nan, 0., 0., 0., 0., 0., nan, 0., 0., 0., nan, nan, nan, 0., nan, nan, nan, nan, nan, nan,\n",
       "        nan, 0., nan, 0., nan, nan, nan, 0., 0., nan, 0., 0., 0., 0., 0., 0., nan, nan, nan, 0., nan, nan, 0., 0.,\n",
       "        0., nan, 0., 0., 0., 0., nan, 0., nan, nan, nan, 0., nan, nan, 0., 0., nan, nan, nan, nan, 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., nan, 0., 0., 0., 0., 0., nan, 0., nan, nan, 0., 0., nan, nan, 0., 0., 0., 0., 0.,\n",
       "        nan, nan, 0., 0., 0., 0., nan, 0., 0., nan, nan, nan, 0., 0., 0., 0., nan, nan, nan, 0., nan, 0., 0., 0.,\n",
       "        nan, 0., 0., 0., 0., nan, 0., 0., nan, nan, nan, 0., nan, nan, 0., 0., 0., 0., nan, nan, 0., 0., 0., 0.,\n",
       "        0., nan, nan, 0., 0., nan, nan, nan, 0., 0., 0., 0., 0., nan, 0., nan, 0., nan, 0., 0., nan, nan, nan, nan,\n",
       "        nan, 0., 0., nan, nan, 0., nan, nan, 0., nan, nan, nan, nan, nan, 0., nan, 0., 0., nan, 0., nan, 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., nan, 0., 0., nan, 0., 0., 0., nan, 0., 0., nan, 0., 0., 0., nan, nan, nan, 0., nan,\n",
       "        0., 0., 0., nan, nan, 0., nan, 0., nan, nan, 0., 0., nan, nan, 0., nan, 0., 0., 0., 0., nan, 0., 0., nan,\n",
       "        0., 0., nan, nan, 0., nan, 0., nan, 0., 0., nan, nan, nan, nan, nan, nan, nan, 0., 0., 0., 0., 0., nan],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(f)/(1+torch.exp(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan, nan,  ..., nan, nan, nan], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.exp(f)/(1+torch.exp(f)) - ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
