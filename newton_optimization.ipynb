{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data_utils\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from torch.utils.data import DataLoader\n",
    "from model import WideModel\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# pip install ucimlrepo\n",
    "\n",
    "# fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = torch.tensor(breast_cancer_wisconsin_diagnostic.data.features.values, dtype=torch.float32)\n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets\n",
    "y[\"Diagnosis\"] = y[\"Diagnosis\"].map({\"M\": 1, \"B\": 0})\n",
    "y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "train = data_utils.TensorDataset(X[:455], y[:455])\n",
    "test = data_utils.TensorDataset(X[455:], y[455:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders. Batch size must be 1\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create device\n",
    "dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"using device\", dev)\n",
    "\n",
    "# create model\n",
    "model = WideModel(input_dim=30,hidden_dim_scale = 20, output_dim=1).to(dev)\n",
    "\n",
    "# create optimizer\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.SGD([p for p in model.parameters()], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated=True\n",
    "while updated:\n",
    "    # get linearized models:\n",
    "    num_params = len(model.flatten_parameters())\n",
    "\n",
    "    # we reduce f(x,w) to Aw+B, where there is a different A,B per x\n",
    "\n",
    "    As = torch.empty((0,num_params))\n",
    "    Bs = torch.empty((0,))\n",
    "    ys = torch.empty((0,))\n",
    "\n",
    "    for x,y in tqdm(train_dataloader):\n",
    "        x = x.to(dev)\n",
    "        \n",
    "        # A = gradient matrix of logits\n",
    "        A = model.flatten_gradient(x).unsqueeze(0)\n",
    "        # print(A.shape)\n",
    "        As = torch.concat((As, A), dim=0)\n",
    "        \n",
    "        # B = f(x,w) - A w\n",
    "        B = model.forward(x) - A @ model.flatten_parameters()\n",
    "        Bs = torch.concat((Bs, B[:,0]), dim=0)\n",
    "        \n",
    "        ys = torch.concat((ys, y[:,0]), dim=0)\n",
    "        \n",
    "        model.Adict[x] = A\n",
    "        model.Bdict[x] = B\n",
    "        \n",
    "    model.update_stored_linear_tensors(As, Bs)\n",
    "    \n",
    "    f = model.batched_linearized_forward(model.flatten_parameters())\n",
    "    updated=False\n",
    "    while f.abs().max() > 1:\n",
    "        updated=True\n",
    "        w = model.flatten_parameters()/2\n",
    "        model.update_parameters(w)\n",
    "        f = model.batched_linearized_forward(model.flatten_parameters())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.w0\n",
    "print(w)\n",
    "for step in range(100):\n",
    "    print(f\"Starting Newton step {step}\")\n",
    "    old_w = w\n",
    "    w = model.newton_update(w, ys)\n",
    "    # print(w)\n",
    "    print(f\"change in w is: {torch.linalg.norm(w-old_w)}\")\n",
    "    # print(f\"Achieved loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = model.batched_linearized_forward(w)\n",
    "dl = (torch.exp(f)/(1+torch.exp(f)) - ys) @ model.Atensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.exp(f[3])/(1+torch.exp(f[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(f)/(1+torch.exp(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.exp(f)/(1+torch.exp(f)) - ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
